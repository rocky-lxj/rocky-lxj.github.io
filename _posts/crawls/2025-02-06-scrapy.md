---
layout: post
title: "scrapy"
date: 2025-02-06 10:36:36 +0000
categories: [scrapy框架]
tags: [python,爬虫,scrapy]
published: true
author : "rocky-lxj"
---

## 创建一个新的项目
```
scrapy startproject ProjectName
```

## scrapy架构
![scrapy](https://github.com/rocky-lxj/rocky-lxj.github.io/raw/main/src/img/about/scrapy-jiagou.png)
```
Scrapy Engine(引擎): 负责Spider、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等。

Scheduler(调度器): 它负责接受引擎发送过来的Request请求，并按照一定的方式进行整理排列，入队，当引擎需要时，交还给引擎。

Downloader（下载器）：负责下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine(引擎)，由引擎交给Spider来处理，

Spider（爬虫）：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给引擎，再次进入Scheduler(调度器).

Item Pipeline(管道)：它负责处理Spider中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方。

Downloader Middlewares（下载中间件）：你可以当作是一个可以自定义扩展下载功能的组件。

Spider Middlewares（Spider中间件）：你可以理解为是一个可以自定扩展和操作引擎和Spider中间通信的功能组件（比如进入Spider的Responses;和从Spider出去的Requests）
```
## 生成爬虫
scrapy genspider +SpiderName+website

## 生成后的模块
```markdown
scray项目名称/
├── scrapy.cfg              # 部署配置文件，用于在不同环境中部署 Scrapy 项目
└── scray项目名称/          # 项目的 Python 模块，所有的代码都放在这个目录下
    ├── __init__.py         
    ├── items.py            # 定义项目中的数据结构（Item），用于存储爬取到的数据
    ├── middlewares.py      # 包含项目的中间件，中间件可用于处理请求和响应
    ├── pipelines.py        # 定义数据处理管道，用于对爬取到的数据进行清洗、存储等操作
    ├── settings.py         # 项目的配置文件，可设置爬取的各种参数，如请求头、下载延迟等
    └── spiders/            # 存放所有的爬虫文件
        ├── __init__.py     # 使 spiders 成为一个 Python 包
        └── douban.py       # 具体的爬虫文件，定义了如何进行爬取和解析数据
```

## 修改setting文件
```
1、修改user-agent 请求 
2、并发数量请求 CONCURRENT_REQUESTS = (一般为2的次方)
3、DOWNLOAD_DELAY = (下载延迟)
4、支持随机RANDOMIZE_DOWNLOAD_DELAY = True
5、ROBOTSTXT_OBEY = True 遵守爬虫协议
```

## 运行(crawl)             
```
-o output 就是将输出结果放在什么地方
scrapy crawl +SpiderName
scrapy crawl SpiderName -o file.json
scrapy crawl SpiderName-o file.csv
```

（4）检查spider文件是否有语法错误
scrapy check

（5）list返回项目所有spider名称
scrapy list

（6）测试电脑当前爬取速度性能：
scrapy bench

（7）scrapy runspider
scrapy runspider zufang_spider.py

（8）编辑spider文件：
scrapy edit <spider>
相当于打开vim模式，实际并不好用，在IDE中编辑更为合适。

（9）将网页内容下载下来，然后在终端打印当前返回的内容，相当于 request 和 urllib 方法：
scrapy fetch <url>

（10）将网页内容保存下来，并在浏览器中打开当前网页内容，直观呈现要爬取网页的内容:　
scrapy view <url>

（11）进入终端。打开 scrapy 显示台，类似ipython，可以用来做测试：
scrapy shell [url]

（12）输出格式化内容：
scrapy parse <url> [options]

（13）返回系统设置信息：
scrapy settings [options]
如：
$ scrapy settings --get BOT_NAME
scrapybot

（14）显示scrapy版本：
scrapy version [-v]
后面加 -v 可以显示scrapy依赖库的版本


scrapy.cfg    项目的主配置信息。（真正爬虫相关的配置信息在settings.py文件中）

items.py      设置数据存储模板，用于结构化数据，如：Django的Model

pipelines     数据持久化处理

settings.py   配置文件

spiders       爬虫目录
